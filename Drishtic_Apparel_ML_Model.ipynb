{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BUILDING APPAREL DETECTION ML MODEL USING NVIDIAâ€™S TRANSFER LEARNING TOOLKIT (TLT)\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where a model trained on one task is re-trained to use on a different task.\n",
    "\n",
    "We will be using DetectNet_v2 - NVIDIA-developed object-detection model for TLT which supports subtasks such as train, prune, evaluate and export.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up env variables for cleaner command line commands.\r\n",
    "%env KEY=tlt_apparels\r\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tlt-experiments/detectnet_v2\r\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tlt-experiments\r\n",
    "%env SPECS_DIR=/workspace/examples/detectnet_v2/specs\r\n",
    "%env NUM_GPUS=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our dataset is mostly from open source images and videos. \r\n",
    "\r\n",
    "The data downloaded from all the sources and converted into kitti format then distributed as follows -\r\n",
    "* training images in `$DATA_DOWNLOAD_DIR/training/image_2`\r\n",
    "* training labels in `$DATA_DOWNLOAD_DIR/training/label_2`\r\n",
    "* testing images in `$DATA_DOWNLOAD_DIR/testing/image_2`\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A. Verify downloaded dataset <a class=\"anchor\" id=\"head-1-1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# verify kitti dataset count \r\n",
    "import os\r\n",
    "\r\n",
    "DATA_DIR = os.environ.get('DATA_DOWNLOAD_DIR')\r\n",
    "num_training_images = len(os.listdir(os.path.join(DATA_DIR, \"training/image_2\")))\r\n",
    "num_training_labels = len(os.listdir(os.path.join(DATA_DIR, \"training/label_2\")))\r\n",
    "num_testing_images = len(os.listdir(os.path.join(DATA_DIR, \"testing/image_2\")))\r\n",
    "print(\"Number of images in the trainval set. {}\".format(num_training_images))\r\n",
    "print(\"Number of labels in the trainval set. {}\".format(num_training_labels))\r\n",
    "print(\"Number of images in the test set. {}\".format(num_testing_images))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B. Prepare tf records from kitti format dataset <a class=\"anchor\" id=\"head-1-2\"></a>\n",
    "\n",
    "* By using tfrecords spec file(detectnet_v2_tfrecords_kitti_trainval.txt) converting it to kitti format dataset\n",
    "\n",
    "*Note: TfRecords only need to be generated once.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a new directory for the output tfrecords dump.\r\n",
    "print(\"Converting Tfrecords for kitti trainval dataset\")\r\n",
    "!detectnet_v2 dataset_convert \\\r\n",
    "              -d $SPECS_DIR/detectnet_v2_tfrecords_kitti_trainval.txt \\\r\n",
    "              -o $DATA_DOWNLOAD_DIR/tfrecords/kitti_trainval/kitti_trainval \r\n",
    "\r\n",
    "# Verify tfrecords count\r\n",
    "!ls -rlt $DATA_DOWNLOAD_DIR/tfrecords/kitti_trainval/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### C. Download pre-trained model <a class=\"anchor\" id=\"head-1-3\"></a>\n",
    "Download the correct pretrained model from the NGC model registry. For optimum results please download model templates from `nvidia/tlt_pretrained_detectnet_v2`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# List models available in the model registry.\n",
    "!ngc registry model list nvidia/tlt_pretrained_detectnet_v2:*"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the target destination to download the model.\n",
    "!mkdir -p $USER_EXPERIMENT_DIR/pretrained_resnet18/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download the pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tlt_pretrained_detectnet_v2:resnet18 \\\n",
    "    --dest $USER_EXPERIMENT_DIR/pretrained_resnet18"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Verify model is downloded to directory\n",
    "!ls -rlt $USER_EXPERIMENT_DIR/pretrained_resnet18/tlt_pretrained_detectnet_v2_vresnet18\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Provide training specification <a class=\"anchor\" id=\"head-2\"></a>\n",
    " \n",
    " Keep specification file at $SPECS_DIR/apparel_train_resnet18_kitti.txt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Run TLT training <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 train -e $SPECS_DIR/apparel_train_resnet18_kitti.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/apparel_unpruned \\\n",
    "                        -k $KEY \\\n",
    "                        -n resnet18_detector \\\n",
    "                        --gpus $NUM_GPUS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Evaluate the trained model <a class=\"anchor\" id=\"head-4\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 evaluate -e $SPECS_DIR/apparel_train_resnet18_kitti.txt\\\n",
    "                       -m $USER_EXPERIMENT_DIR/apparel_unpruned/weights/resnet18_detector.tlt \\\n",
    "                       -k $KEY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Prune the trained model <a class=\"anchor\" id=\"head-5\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an output directory if it doesn't exist.\n",
    "!mkdir -p $USER_EXPERIMENT_DIR/apparel_pruned"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 prune \\\n",
    "              -m $USER_EXPERIMENT_DIR/apparel_unpruned/weights/resnet18_detector.tlt \\\n",
    "              -o $USER_EXPERIMENT_DIR/apparel_pruned/resnet18_nopool_bn_detectnet_v2_pruned.tlt \\\n",
    "              -eq union \\\n",
    "              -pth 0.0000052 \\\n",
    "              -k $KEY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Retrain the pruned model <a class=\"anchor\" id=\"head-6\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!detectnet_v2 train -e $SPECS_DIR/apparel_retrain_resnet18_kitti.txt \\\n",
    "                    -r $USER_EXPERIMENT_DIR/apparel_retrain \\\n",
    "                    -k $KEY \\\n",
    "                    -n apparel_pruned \\\n",
    "                    --gpus $NUM_GPUS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Evaluate the retrained model <a class=\"anchor\" id=\"head-7\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section evaluates the pruned and retrained model, using `tlt-evaluate`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 evaluate -e $SPECS_DIR/apparel_retrain_resnet18_kitti.txt \\\n",
    "                       -m $USER_EXPERIMENT_DIR/apparel_retrain/weights/resnet18_detector_pruned.tlt \\\n",
    "                       -k $KEY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Visualize inferences <a class=\"anchor\" id=\"head-8\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Running inference for detection on n images\n",
    "!detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_kitti_tlt.txt \\\n",
    "                        -o $USER_EXPERIMENT_DIR/tlt_infer_testing \\\n",
    "                        -i $DATA_DOWNLOAD_DIR/testing/image_2 \\\n",
    "                        -k $KEY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Simple grid visualizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['USER_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizing the first 12 images.\n",
    "OUTPUT_PATH = 'tlt_infer_testing/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 16 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Deploy! <a class=\"anchor\" id=\"head-9\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mkdir -p $USER_EXPERIMENT_DIR/experiment_dir_final15\n",
    "# Removing a pre-existing copy of the etlt if there has been any.\n",
    "import os\n",
    "output_file=os.path.join(os.environ['USER_EXPERIMENT_DIR'],\n",
    "                         \"apparel_final/resnet18_detector.etlt\")\n",
    "if os.path.exists(output_file):\n",
    "    os.system(\"rm {}\".format(output_file))\n",
    "!detectnet_v2 export \\\n",
    "            -m $USER_EXPERIMENT_DIR/aparel_retrain/weights/resnet18_detector_pruned.tlt \\\n",
    "            -o $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector.etlt \\\n",
    "            -k $KEY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A. Int8 Optimization <a class=\"anchor\" id=\"head-9-1\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!rm -rf $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector.etlt\n",
    "!rm -rf $USER_EXPERIMENT_DIR/apparel_final/calibration.bin\n",
    "!detectnet_v2 export \\\n",
    "              -m $USER_EXPERIMENT_DIR/apparel_retrain/weights/resnet18_detector_pruned13.tlt \\\n",
    "              -o $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector.etlt \\\n",
    "              -k $KEY  \\\n",
    "              --cal_data_file $USER_EXPERIMENT_DIR/apparel_final/calibration.tensor \\\n",
    "              --data_type int8 \\\n",
    "              --batches 10 \\\n",
    "              --batch_size 4 \\\n",
    "              --max_batch_size 4\\\n",
    "              --engine_file $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector.trt.int8 \\\n",
    "              --cal_cache_file $USER_EXPERIMENT_DIR/apparel_final/calibration.bin \\\n",
    "              --verbose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B. Generate TensorRT engine <a class=\"anchor\" id=\"head-9-2\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!tlt-converter $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector.etlt \\\n",
    "               -k $KEY \\\n",
    "               -c $USER_EXPERIMENT_DIR/apparel_final/calibration.bin \\\n",
    "               -o output_cov/Sigmoid,output_bbox/BiasAdd \\\n",
    "               -d 3,256,256 \\\n",
    "               -i nchw \\\n",
    "               -m 64 \\\n",
    "               -t int8 \\\n",
    "               -e $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector.trt \\\n",
    "               -b 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. QAT workflow <a class=\"anchor\" id=\"head-11\"></a>\n",
    "This section delves into the newly enabled Quantization Aware Training feature with DetectNet_v2. The workflow defined below converts a pruned model from section [5](#head-5). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A. Convert pruned model to QAT and retrain <a class=\"anchor\" id=\"head-11-1\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                    -r $USER_EXPERIMENT_DIR/apparel_retrain_qat \\\n",
    "                    -k $KEY \\\n",
    "                    -n resnet18_detector_pruned_qat \\\n",
    "                    --gpus $NUM_GPUS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B. Evaluate QAT converted model <a class=\"anchor\" id=\"head-11-2\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                       -m $USER_EXPERIMENT_DIR/final_retrain_qat/weights/resnet18_detector_pruned_qat.tlt \\\n",
    "                       -k $KEY \\\n",
    "                       -f tlt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### C. Export QAT trained model to int8 <a class=\"anchor\" id=\"head-11-3\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!rm -rf $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector_qat.etlt\n",
    "!rm -rf $USER_EXPERIMENT_DIR/apparel_final/calibration_qat.bin\n",
    "!detectnet_v2 export \\\n",
    "              -m $USER_EXPERIMENT_DIR/apparel_retrain/weights/resnet18_detector_pruned_qat.tlt \\\n",
    "              -o $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector_qat.etlt \\\n",
    "              -k $KEY  \\\n",
    "              --data_type int8 \\\n",
    "              --batch_size 64 \\\n",
    "              --max_batch_size 64\\\n",
    "              --engine_file $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector_qat.trt.int8 \\\n",
    "              --cal_cache_file $USER_EXPERIMENT_DIR/apparel_final/calibration_qat.bin \\\n",
    "              --verbose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### D. Evaluate a QAT trained model using the exported TensorRT engine <a class=\"anchor\" id=\"head-11-4\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                       -m $USER_EXPERIMENT_DIR/apparel_final/resnet18_detector_qat.trt.int8 \\\n",
    "                       -f tensorrt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### D. Inference using QAT engine <a class=\"anchor\" id=\"head-11-5\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_kitti_etlt_qat.txt \\\n",
    "                        -o $USER_EXPERIMENT_DIR/tlt_infer_testing_qat \\\n",
    "                        -i $DATA_DOWNLOAD_DIR/testing/image_2 \\\n",
    "                        -k $KEY"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}